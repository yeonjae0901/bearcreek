#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time
import datetime
import logging
import schedule
import asyncio
import sys
import pytz
import json
import random
import re
import cloudscraper
from pathlib import Path
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from telegram import Bot
from telegram.error import TelegramError
from playwright.async_api import async_playwright

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("ultimate_checker.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¤ì • ì •ë³´
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
CHECK_INTERVAL_MINUTES = int(os.getenv('CHECK_INTERVAL_MINUTES', 5).replace('%', ''))
MONTH = int(os.getenv('MONTH', 4).replace('%', ''))
YEAR = int(os.getenv('YEAR', 2025).replace('%', ''))

# ë² ì–´í¬ë¦¬í¬ ê³¨í”„ì¥ URL ì •ë³´
BEARCREEK_URL = "https://www.bearcreek.co.kr/Reservation/Reservation.aspx?strLGubun=110&strClubCode=N#aCourseSel"
BEARCREEK_API_URL = "https://www.bearcreek.co.kr/Reservation/XmlCalendarData.aspx"

# í´ë¼ìš°ë“œìŠ¤í¬ë ˆì´í¼ ì„¸ì…˜ (ì „ì—­ë³€ìˆ˜)
scraper = None

# ì‚¬ìš©ì ì—ì´ì „íŠ¸ ëª©ë¡
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

# í…ŒìŠ¤íŠ¸ìš© ì¿ í‚¤ê°€ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ê²½ë¡œ
COOKIES_FILE = 'bearcreek_cookies.json'

async def send_telegram_message(message):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë°œì†¡ í•¨ìˆ˜"""
    if not all([TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID]):
        logger.error("í…”ë ˆê·¸ë¨ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return False
    
    try:
        logger.info(f"í…”ë ˆê·¸ë¨ ë´‡ í† í° ìœ íš¨ì„± ê²€ì‚¬ ì¤‘... (ê¸¸ì´: {len(TELEGRAM_BOT_TOKEN)})")
        logger.info(f"í…”ë ˆê·¸ë¨ ì±„íŒ… ID ìœ íš¨ì„± ê²€ì‚¬ ì¤‘... (ê°’: {TELEGRAM_CHAT_ID})")
        
        bot = Bot(token=TELEGRAM_BOT_TOKEN)
        logger.info("í…”ë ˆê·¸ë¨ ë´‡ ê°ì²´ ìƒì„± ì™„ë£Œ")
        
        # ì±„íŒ… ID ì²˜ë¦¬
        chat_id = TELEGRAM_CHAT_ID
        if isinstance(chat_id, str):
            if chat_id.startswith('-') and chat_id[1:].isdigit():
                chat_id = int(chat_id)
            elif chat_id.isdigit():
                chat_id = int(chat_id)
        
        response = await bot.send_message(chat_id=chat_id, text=message, parse_mode='HTML')
        logger.info(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ê°€ ì„±ê³µì ìœ¼ë¡œ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. ë©”ì‹œì§€ ID: {response.message_id}")
        return True
    except TelegramError as e:
        logger.error(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return False

def send_telegram_notification(message):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ë°œì†¡ì„ ìœ„í•œ ë™ê¸° ë˜í¼ í•¨ìˆ˜"""
    asyncio.run(send_telegram_message(message))

def get_random_user_agent():
    """ë¬´ì‘ìœ„ ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„ íƒ"""
    return random.choice(USER_AGENTS)

def load_cookies_from_file():
    """ì €ì¥ëœthe ì¿ í‚¤ íŒŒì¼ì—ì„œ ì¿ í‚¤ ë¡œë“œ"""
    try:
        if os.path.exists(COOKIES_FILE):
            with open(COOKIES_FILE, 'r') as f:
                return json.load(f)
        else:
            logger.warning(f"ì¿ í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {COOKIES_FILE}")
            return []
    except Exception as e:
        logger.error(f"ì¿ í‚¤ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []

def save_cookies_to_file(cookies):
    """ì¿ í‚¤ë¥¼ íŒŒì¼ì— ì €ì¥"""
    try:
        with open(COOKIES_FILE, 'w') as f:
            json.dump(cookies, f)
        logger.info(f"ì¿ í‚¤ê°€ {COOKIES_FILE}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì¿ í‚¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

async def generate_cookies_with_playwright():
    """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿ í‚¤ ìƒì„±"""
    logger.info("Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆ ì¿ í‚¤ ìƒì„± ì¤‘...")
    
    playwright = None
    browser = None
    context = None
    page = None
    
    try:
        playwright = await async_playwright().start()
        browser_type = playwright.chromium
        
        # ë¸Œë¼ìš°ì € ì‹œì‘ (ìŠ¤í…”ìŠ¤ ëª¨ë“œ)
        browser = await browser_type.launch(
            headless=True,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-extensions',
                '--no-sandbox',
                '--disable-web-security',
            ]
        )
        
        # ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=get_random_user_agent(),
            locale='ko-KR',
            timezone_id='Asia/Seoul',
            java_script_enabled=True
        )
        
        # WebDriver ì†ì„± ë®ì–´ì“°ê¸° ë“± ìŠ¤í…”ìŠ¤ ê¸°ëŠ¥ ì¶”ê°€
        await context.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {
          get: () => false,
        });
        
        if(!window.chrome) {
          window.chrome = {
            runtime: {},
          };
        }
        
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters) => (
          parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission }) :
            originalQuery(parameters)
        );
        """)
        
        # í˜ì´ì§€ ìƒì„± ë° íƒ€ì„ì•„ì›ƒ ì„¤ì •
        page = await context.new_page()
        page.set_default_navigation_timeout(60000)
        
        # ë©”ì¸ í˜ì´ì§€ ì ‘ì†
        logger.info(f"ë² ì–´í¬ë¦¬í¬ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘: {BEARCREEK_URL}")
        await page.goto(BEARCREEK_URL, wait_until='networkidle')
        
        # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° (ì¶”ê°€ ì—¬ìœ )
        await page.wait_for_timeout(5000)
        
        # CF ìš°íšŒë¥¼ ìœ„í•œ ì¸í„°ë™ì…˜ ì‹œë®¬ë ˆì´ì…˜ (ë§ˆìš°ìŠ¤ ì´ë™ ë° ìŠ¤í¬ë¡¤)
        await page.mouse.move(random.randint(100, 500), random.randint(100, 500))
        # ë§ˆìš°ìŠ¤ íœ  ìŠ¤í¬ë¡¤ (delta_x, delta_y) - xì¶• ìŠ¤í¬ë¡¤ì€ 0ìœ¼ë¡œ ì„¤ì •
        await page.mouse.wheel(0, random.randint(100, 300))
        await page.wait_for_timeout(1000)
        
        # í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸
        content = await page.content()
        if "Checking your browser" in content or "cloudflare" in content.lower():
            logger.info("Cloudflare í™•ì¸ í™”ë©´ ê°ì§€ë¨, ì¶”ê°€ ëŒ€ê¸° ì¤‘...")
            await page.wait_for_timeout(10000)  # ì¶”ê°€ ëŒ€ê¸°
        
        # ì¿ í‚¤ ì¶”ì¶œ
        cookies = await context.cookies()
        logger.info(f"ìƒì„±ëœ ì¿ í‚¤ ê°œìˆ˜: {len(cookies)}")
        
        # ì¿ í‚¤ ì €ì¥
        if cookies:
            save_cookies_to_file(cookies)
            return cookies
        else:
            logger.warning("ì¿ í‚¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
    except Exception as e:
        logger.error(f"Playwrightë¡œ ì¿ í‚¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []
    finally:
        if page:
            await page.close()
        if context:
            await context.close()
        if browser:
            await browser.close()
        if playwright:
            await playwright.stop()

def setup_cloudscraper():
    """CloudScraper ì„¤ì •"""
    global scraper
    
    try:
        # ì´ë¯¸ ìƒì„±ëœ ê²½ìš° ì¬ì‚¬ìš©
        if scraper:
            return scraper
        
        # ìƒˆë¡œìš´ CloudScraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        logger.info("CloudScraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
        scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            },
            delay=5,  # ê° ìš”ì²­ ì‚¬ì´ ì§€ì—° ì‹œê°„
            debug=False
        )
        
        # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì •
        scraper.headers.update({
            'User-Agent': get_random_user_agent(),
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Referer': 'https://www.bearcreek.co.kr/',
        })
        
        # ì €ì¥ëœ ì¿ í‚¤ ë¡œë“œ
        cookies = load_cookies_from_file()
        if cookies:
            logger.info(f"ì €ì¥ëœ ì¿ í‚¤ {len(cookies)}ê°œ ë¡œë“œë¨")
            # CloudScraper ìš”êµ¬ í˜•ì‹ìœ¼ë¡œ ì¿ í‚¤ ë³€í™˜
            for cookie in cookies:
                scraper.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
        else:
            logger.warning("ì €ì¥ëœ ì¿ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿ í‚¤ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            # ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œì„ ìœ„í•œ ì„ì‹œ ì‹¤í–‰
            new_cookies = asyncio.run(generate_cookies_with_playwright())
            if new_cookies:
                for cookie in new_cookies:
                    scraper.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
        
        return scraper
    except Exception as e:
        logger.error(f"CloudScraper ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def extract_valid_dates(html_content):
    """HTMLì—ì„œ ì˜ˆì•½ ê°€ëŠ¥í•œ ë‚ ì§œ ì¶”ì¶œ"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        available_dates = []
        
        # ë‹¬ë ¥ í…Œì´ë¸” ì°¾ê¸°
        calendar_table = soup.select_one('table.calendar')
        if not calendar_table:
            logger.warning("ë‹¬ë ¥ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # í´ë¦­ ê°€ëŠ¥í•œ ë‚ ì§œ ì°¾ê¸° (onclick ì†ì„±ì´ ìˆëŠ” td)
        date_cells = calendar_table.select('td[onclick]')
        logger.info(f"ë°œê²¬ëœ ë‚ ì§œ ì…€: {len(date_cells)}ê°œ")
        
        for cell in date_cells:
            # ì˜ˆì•½ ë¶ˆê°€ëŠ¥í•œ ë‚ ì§œ ì œì™¸ (classì— 'red'ê°€ í¬í•¨ëœ ê²½ìš°)
            if 'red' in cell.get('class', []):
                continue
            
            # ë‚ ì§œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            date_text = cell.text.strip()
            if date_text and date_text.isdigit():
                day = int(date_text)
                date_str = f"{YEAR}-{MONTH:02d}-{day:02d}"
                available_dates.append(date_str)
                logger.info(f"ì˜ˆì•½ ê°€ëŠ¥í•œ ë‚ ì§œ ë°œê²¬: {date_str}")
        
        return available_dates
    except Exception as e:
        logger.error(f"HTMLì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []

def check_available_dates():
    """ë² ì–´í¬ë¦¬í¬ ê³¨í”„ì¥ ì˜ˆì•½ ê°€ëŠ¥ ë‚ ì§œ í™•ì¸ (CloudScraper ì‚¬ìš©)"""
    logger.info("ë² ì–´í¬ë¦¬í¬ ê³¨í”„ì¥ ì˜ˆì•½ í™•ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # CloudScraper ì„¤ì •
    global scraper
    scraper = setup_cloudscraper()
    if not scraper:
        logger.error("CloudScraper ì„¤ì • ì‹¤íŒ¨")
        return False
    
    try:
        # 1. ë©”ì¸ í˜ì´ì§€ ì ‘ì† (ì¿ í‚¤ ë° í† í° ìˆ˜ì§‘)
        logger.info(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¤‘: {BEARCREEK_URL}")
        
        # Cloudflare ìš°íšŒë¥¼ ìœ„í•œ ì¶”ê°€ í—¤ë”
        scraper.headers.update({
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'sec-ch-ua': '"Chromium";v="110", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        })
        
        response = scraper.get(BEARCREEK_URL, timeout=30)
        if response.status_code != 200:
            logger.error(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
            
            # ì‘ë‹µ ë‚´ìš© ì €ì¥ (ë””ë²„ê¹…ìš©)
            with open("cloudflare_challenge.html", "w", encoding="utf-8") as f:
                f.write(response.text)
            logger.info("ì‘ë‹µ ë‚´ìš©ì´ cloudflare_challenge.htmlì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # Cloudflare ìš°íšŒ ì‹¤íŒ¨ ì‹œ ìƒˆ ì¿ í‚¤ ìƒì„± ì‹œë„
            new_cookies = asyncio.run(generate_cookies_with_playwright())
            if new_cookies:
                for cookie in new_cookies:
                    scraper.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
                
                # ì¬ì‹œë„
                logger.info("ìƒˆ ì¿ í‚¤ë¡œ ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì¬ì‹œë„ ì¤‘...")
                response = scraper.get(BEARCREEK_URL, timeout=30)
                if response.status_code != 200:
                    logger.error(f"ì¬ì‹œë„ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
                    return False
            else:
                return False
        
        # ì‘ë‹µ ë‚´ìš© ì €ì¥ (ë””ë²„ê¹…ìš©)
        with open("main_page.html", "w", encoding="utf-8") as f:
            f.write(response.text)
        logger.info("ë©”ì¸ í˜ì´ì§€ ë‚´ìš©ì´ main_page.htmlì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # 2. AJAX ìš”ì²­ì„ í†µí•´ ìº˜ë¦°ë” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        # ìš”ì²­ íŒŒë¼ë¯¸í„° ì„¤ì •
        calendar_params = {
            'strClubCode': 'N', 
            'strLGubun': '110',
            'strReserveDate': f'{YEAR}-{MONTH:02d}-01'
        }
        
        # AJAX ìš”ì²­ í—¤ë” ì„¤ì • (ë” ìì—°ìŠ¤ëŸ¬ìš´ ìš”ì²­ í‰ë‚´)
        ajax_headers = {
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': BEARCREEK_URL,
            'Origin': 'https://www.bearcreek.co.kr',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
        }
        
        # ì¼ë‹¨ ë©”ì¸ í˜ì´ì§€ HTMLë¡œ ì§ì ‘ ë‹¬ë ¥ íŒŒì‹± ì‹œë„
        available_dates = extract_valid_dates(response.text)
        if available_dates:
            # ë‹¬ë ¥ ë°ì´í„°ê°€ ë©”ì¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œë˜ë©´ API í˜¸ì¶œ ë¶ˆí•„ìš”
            logger.info("ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‹¬ë ¥ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ")
        else:
            # ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‹¬ë ¥ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ API í˜¸ì¶œ ì‹œë„
            logger.info(f"ìº˜ë¦°ë” ë°ì´í„° ìš”ì²­ ì¤‘: {BEARCREEK_API_URL}")
            try:
                calendar_response = scraper.post(
                    BEARCREEK_API_URL, 
                    headers=ajax_headers,
                    data=calendar_params,
                    timeout=30
                )
                
                if calendar_response.status_code != 200:
                    logger.error(f"ìº˜ë¦°ë” ë°ì´í„° ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {calendar_response.status_code}")
                else:
                    # ì‘ë‹µ ë‚´ìš© ì €ì¥ (ë””ë²„ê¹…ìš©)
                    with open(f"calendar_data_{YEAR}_{MONTH:02d}.html", "w", encoding="utf-8") as f:
                        f.write(calendar_response.text)
                    logger.info(f"ìº˜ë¦°ë” ë°ì´í„°ê°€ calendar_data_{YEAR}_{MONTH:02d}.htmlì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    
                    # ì‘ë‹µëœ XML/HTMLì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                    soup = BeautifulSoup(calendar_response.text, 'html.parser')
                    logger.info(f"ìº˜ë¦°ë” ë°ì´í„° ê¸¸ì´: {len(calendar_response.text)}")
                    
                    # ë””ë²„ê¹…ìš© API ì‘ë‹µ ì¶œë ¥
                    preview = calendar_response.text[:200].replace('\n', ' ')
                    logger.info(f"ìº˜ë¦°ë” API ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {preview}...")
            except Exception as e:
                logger.error(f"ìº˜ë¦°ë” API ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        
        # 4. ê²°ê³¼ ì²˜ë¦¬
        if available_dates:
            logger.info(f"ì´ {len(available_dates)}ê°œì˜ ì˜ˆì•½ ê°€ëŠ¥ ë‚ ì§œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # ì•Œë¦¼ ë©”ì‹œì§€ êµ¬ì„±
            message = f"ğŸŒï¸ <b>ë² ì–´í¬ë¦¬í¬ ì˜ˆì•½ ì•Œë¦¼</b>\n\n"
            message += f"{YEAR}ë…„ {MONTH}ì›”ì— ë‹¤ìŒ ë‚ ì§œì— ì˜ˆì•½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:\n\n"
            
            for date_str in available_dates:
                message += f"- {date_str}\n"
            
            message += f"\nì˜ˆì•½ í˜ì´ì§€: {BEARCREEK_URL}"
            
            # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì „ì†¡
            send_telegram_notification(message)
            return True
        else:
            logger.info(f"{YEAR}ë…„ {MONTH}ì›”ì— ì˜ˆì•½ ê°€ëŠ¥í•œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        logger.error(f"ì˜ˆì•½ í™•ì¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        return False

def run_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    logger.info(f"ë² ì–´í¬ë¦¬í¬ ì˜ˆì•½ í™•ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. í™•ì¸ ì£¼ê¸°: {CHECK_INTERVAL_MINUTES}ë¶„")
    
    # ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰
    check_available_dates()
    
    # ìŠ¤ì¼€ì¤„ ì„¤ì •
    schedule.every(CHECK_INTERVAL_MINUTES).minutes.do(check_available_dates)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ë¬´í•œ ë£¨í”„
    try:
        while True:
            schedule.run_pending()
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("Ctrl+Cë¡œ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    finally:
        logger.info("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    logger.info("ë² ì–´í¬ë¦¬í¬ ì˜ˆì•½ í™•ì¸ ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--single":
        # ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œ
        logger.info("ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        check_available_dates()
    else:
        # ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“œ
        run_scheduler() 